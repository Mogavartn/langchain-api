import os
from langchain.memory import ConversationBufferMemory
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
import json

# R√©cup√©ration de la cl√© API depuis une variable d‚Äôenvironnement
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# Initialisation de la m√©moire (stock√©e en m√©moire temporaire, non persistante)
memory_store = {}

# Donn√©es des blocs (remplace par ton JSON complet)
blocs = [
    {"id": "legalite_programme", "response": "On ne peut pas inscrire une personne dans une formation si son but est d'√™tre r√©mun√©r√©e pour √ßa. En revanche, si tu fais la formation s√©rieusement, tu peux ensuite participer au programme d'affiliation et parrainer d'autres personnes."},
    {"id": "sans_reseaux_sociaux", "response": "Pas de souci si tu n'es pas sur Insta ou Snap üòå Tu peux simplement nous envoyer des contacts potentiellement int√©ress√©s. √áa fonctionne tr√®s bien aussi üòâ"},
    {"id": "gestion_agressivite", "response": "√ätre impoli ne fera pas avancer la situation plus vite. Bien au contraire. Souhaites-tu que je te propose un po√®me ou une chanson d'amour pour apaiser ton c≈ìur ? üíå"},
    # Ajoute tous tes blocs ici
]

# Cr√©ation d'un vector store pour la recherche s√©mantique (initialis√© une seule fois)
texts = [bloc["response"] for bloc in blocs]
embeddings = OpenAIEmbeddings()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.create_documents(texts)
vector_store = FAISS.from_documents(docs, embeddings)

def handler(request):
    try:
        body = json.loads(request.body.decode('utf-8'))
        user_message = body.get("message_original", "")
        matched_bloc_response = body.get("matched_bloc_response", "")
        wa_id = body.get("wa_id", "")

        if wa_id not in memory_store:
            memory_store[wa_id] = ConversationBufferMemory()
        memory = memory_store[wa_id]

        memory.chat_memory.add_user_message(user_message)

        if matched_bloc_response:
            memory.chat_memory.add_ai_message(matched_bloc_response)
            return {
                "statusCode": 200,
                "body": json.dumps({
                    "matched_bloc_response": matched_bloc_response,
                    "memory": memory.load_memory_variables({})["history"]
                })
            }

        similar_docs = vector_store.similarity_search(user_message, k=1)
        best_match = None
        for doc in similar_docs:
            for bloc in blocs:
                if bloc["response"] == doc.page_content:
                    best_match = bloc
                    break

        if best_match:
            matched_bloc_response = best_match["response"]
            memory.chat_memory.add_ai_message(matched_bloc_response)
            return {
                "statusCode": 200,
                "body": json.dumps({
                    "matched_bloc_response": matched_bloc_response,
                    "memory": memory.load_memory_variables({})["history"]
                })
            }

        escalade_response = "Je vais faire suivre √† la bonne personne dans l‚Äô√©quipe üòä Notre √©quipe est disponible du lundi au vendredi, de 9h √† 17h (hors pause d√©jeuner)."
        memory.chat_memory.add_ai_message(escalade_response)
        return {
            "statusCode": 200,
            "body": json.dumps({
                "matched_bloc_response": escalade_response,
                "memory": memory.load_memory_variables({})["history"]
            })
        }
    except Exception as e:
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)})
        }